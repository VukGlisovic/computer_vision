{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104910d-ce3a-496b-b12a-68afab737032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gpus = GPUtil.getGPUs()\n",
    "for gpu in gpus:\n",
    "    print(f\"GPU ID: {gpu.id}, GPU Name: {gpu.name}\")\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal} MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree} MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7d3a4-c2e6-45fd-8174-a5a3dc93d04e",
   "metadata": {},
   "source": [
    "# Load DINOv2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c91293-6656-45fc-b01f-548b2db497a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vit14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_lc')\n",
    "# dinov2_vit14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_lc')\n",
    "# dinov2_vit14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_lc')\n",
    "# dinov2_vit14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_lc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9d1a6-b871-44ad-a988-746c05d1c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to GPU\n",
    "dinov2_vit14 = dinov2_vit14.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192855d-9834-43b3-b73f-adf95896e90e",
   "metadata": {},
   "source": [
    "# Load image to run inference on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6352c-2873-4e10-9ed9-b762a3c742ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image from URL\n",
    "response = requests.get('https://m.media-amazon.com/images/M/MV5BMTM1MjQzMDA5NV5BMl5BanBnXkFtZTcwMDk5MDg3Mw@@._V1_.jpg')\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Define the transformation to convert PIL image to a PyTorch tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Apply the transformation to convert PIL image to a PyTorch tensor\n",
    "img = transform(img)  # will make channels_first torch tensor\n",
    "img = img[None,...]  # add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3c835-460e-4e57-b5bd-1d6ca3da5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img, size, preserve_aspect_ratio=True):\n",
    "    assert size[0] % 14 == 0, \"DINOv2 expects input image with shapes that are multiples of 14. Height is not a multiple of 14.\"\n",
    "    assert size[1] % 14 == 0, \"DINOv2 expects input image with shapes that are multiples of 14. Width is not a multiple of 14.\"\n",
    "    \n",
    "    # Define the transformation to resize the image\n",
    "    resize_transform = transforms.Resize(size, antialias=True)\n",
    "\n",
    "    # Apply the resize transformation to the PIL image\n",
    "    resized_image = resize_transform(img)\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111746d6-6a48-4c2d-8fca-ff03d1580c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_height = 14 * 5\n",
    "new_width = round((img.shape[2] / img.shape[3]) * new_height / 14) * 14\n",
    "print(new_height, new_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de87ac8-82ff-4218-be8c-68385ff0121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized = resize(img, (new_height, new_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31caca2-ec5f-4c8e-8db2-a64974ffdae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized = img_resized.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef2ec5-82ed-4e2c-8ce6-4b901c0bfda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dinov2_vit14(img_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b2010-848f-42a0-96f5-00bf1c3a026a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e9e15-02ac-4a2c-bd63-be70fbe3e15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c60d23-7771-4f57-b29e-5c87e06a8ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f044bfd-e094-4003-b850-02fcf7f95c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "dinov2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
