{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16b115e-b85c-450f-b832-babb872a9ee3",
   "metadata": {},
   "source": [
    "This notebook is for running inference with a DINO (v1) model and visualizing the attention values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104910d-ce3a-496b-b12a-68afab737032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import GPUtil\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "gpus = GPUtil.getGPUs()\n",
    "for gpu in gpus:\n",
    "    print(f\"GPU ID: {gpu.id}, GPU Name: {gpu.name}\")\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal} MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree} MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7d3a4-c2e6-45fd-8174-a5a3dc93d04e",
   "metadata": {},
   "source": [
    "# Load DINO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c91293-6656-45fc-b01f-548b2db497a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')\n",
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb8')\n",
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_small_12_p16')\n",
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_small_12_p8')\n",
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_medium_24_p16')\n",
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_medium_24_p8')\n",
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')\n",
    "\n",
    "print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601a331-90bb-496d-8205-689dc328c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = model.patch_embed.proj.kernel_size[0]\n",
    "print(f\"patch_size: {patch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bbe46-b965-4a86-9cf5-662798bd827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model for inference\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192855d-9834-43b3-b73f-adf95896e90e",
   "metadata": {},
   "source": [
    "# Load image to run inference on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3c835-460e-4e57-b5bd-1d6ca3da5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img, size, preserve_aspect_ratio=True):\n",
    "    # Define the transformation to resize the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x[None,...]),\n",
    "        transforms.Resize(size, antialias=True),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    return transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125fcd5e-04bb-4609-a42e-fa92443b40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image from URL\n",
    "url = 'https://m.media-amazon.com/images/M/MV5BMTM1MjQzMDA5NV5BMl5BanBnXkFtZTcwMDk5MDg3Mw@@._V1_.jpg'  # into the wild\n",
    "# url = 'https://static.euronews.com/articles/stories/06/35/53/24/1440x810_cmsv2_548e11b5-0a57-53f4-88d9-5ea703413300-6355324.jpg'  # latest fra ID\n",
    "# url = 'https://upload.wikimedia.org/wikipedia/commons/1/14/New_Estonian_ID_card_%282021%29%28back%29.jpg'  # EST ID back\n",
    "\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# configure the new image height for input into the DINO model\n",
    "new_height = 720\n",
    "new_height = new_height // patch_size * patch_size  # find closest matching to patch size height\n",
    "new_width = round((img.size[0] / img.size[1]) * new_height / patch_size) * patch_size\n",
    "print(new_height, new_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de87ac8-82ff-4218-be8c-68385ff0121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image for inference\n",
    "img = preprocess_image(img, (new_height, new_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bbfd5-116f-4a04-b3b4-c0b76d715e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inference; get the attention values of the [CLS] token of the last multi-head attention layer\n",
    "attentions = model.get_last_selfattention(img.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a0498-2a49-412f-a8bb-5e4c9d6cacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the attention values of interest\n",
    "\n",
    "# number of heads\n",
    "nh = attentions.shape[1]\n",
    "\n",
    "# we keep only the output patch attention; attention from [CLS] token to each image patch\n",
    "attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "# reshape back to original image shape with a factor of 'patch_size' difference\n",
    "w_featmap = img.shape[-2] // patch_size\n",
    "h_featmap = img.shape[-1] // patch_size\n",
    "attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "\n",
    "# repeat the attention values such that they are as big as the original image size\n",
    "attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e5355-c596-4dab-9bc6-72cf3e88e8c8",
   "metadata": {},
   "source": [
    "# Plot image with attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16731e7e-8250-4175-bb60-cf8a7d7ed855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the input image with the attention maps\n",
    "\n",
    "# configure number of columns for the attention maps\n",
    "n_cols = 3\n",
    "n_rows = nh // n_cols + 1  # +1 for the first row where we'll plot the input image\n",
    "assert nh % n_cols == 0  # make sure all attention maps are plotted\n",
    "\n",
    "h, w = img.shape[2:]\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*(h/w)*n_rows))\n",
    "fig.tight_layout()\n",
    "\n",
    "# plot the main image in the top row\n",
    "gs = axes[0, 0].get_gridspec()\n",
    "# remove the underlying axes\n",
    "for ax in axes[0, :]:\n",
    "    ax.remove()\n",
    "ax_top = fig.add_subplot(gs[0, :])\n",
    "main_img = torchvision.utils.make_grid(img, normalize=True, scale_each=True).permute((1,2,0))\n",
    "ax_top.imshow(main_img)\n",
    "\n",
    "# plot the attention maps underneath the main image\n",
    "axes = np.ravel(axes)\n",
    "for i in range(nh):\n",
    "    ax = axes[i + n_cols]\n",
    "    ax.set_title(f\"attention-head{i}\", fontsize=13)\n",
    "    ax.imshow(main_img)\n",
    "    ax.imshow(attentions[i], alpha=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16566d3f-9edc-4624-88a3-b8d35d492517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f044bfd-e094-4003-b850-02fcf7f95c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "dinov2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
