{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unet.model.architecture import create_unet_model\n",
    "from unet.model.constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get image paths and inspect some images\n",
    "\n",
    "Note that the images are the training input features and that the masks are the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_files = glob('../data/images/*')\n",
    "train_mask_files = glob('../data/masks/*')\n",
    "print(\"Number of train files: {}\".format(len(train_image_files)))\n",
    "print(\"Number of train masks: {}\".format(len(train_mask_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_path = train_image_files[-1]\n",
    "sample_image = cv2.imread(sample_image_path)\n",
    "sample_mask_path = train_mask_files[-1]\n",
    "sample_mask = cv2.imread(sample_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image.shape, sample_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_samples = 5\n",
    "fig, axes = plt.subplots(nr_samples, 2, figsize=(10, 5*nr_samples))\n",
    "\n",
    "for (ax_image, ax_mask), image_path, mask_path in zip(axes, train_image_files, train_mask_files):\n",
    "    ax_image.imshow(cv2.imread(image_path)[:,:,0], cmap='seismic', interpolation='bilinear')\n",
    "    ax_mask.imshow(cv2.imread(mask_path)[:,:,0], cmap='gray', interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data\n",
    "\n",
    "### Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = [cv2.imread(p) for p in tqdm(train_image_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = cv2.calcHist([Xtrain[0]], [0], None, [256], [0,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that opencv uses BGR order of channels instead of RGB\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "\n",
    "color = ('b', 'g', 'r')\n",
    "for i, col in enumerate(color):\n",
    "    hist = cv2.calcHist(images=[Xtrain[0]], channels=[i], mask=None, histSize=[256], ranges=[0, 256])\n",
    "    ax.plot(hist, color=col, lw=2, alpha=0.5)\n",
    "ax.set_ylabel(\"count\", fontsize=14)\n",
    "ax.set_xlabel(\"pixel value\", fontsize=14)\n",
    "ax.set_xlim([0,256]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hists = [cv2.calcHist(images=[img[0]], channels=[0], mask=None, histSize=[256], ranges=[0, 256]) for img in Xtrain]\n",
    "all_hists = np.squeeze(np.array(all_hists))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 7))\n",
    "ax.set_title(\"Distribution pixel values training data\", fontsize=16)\n",
    "\n",
    "\n",
    "ax.plot(all_hists.mean(axis=0), color='blue', lw=2)\n",
    "ax.fill_between(range(256), all_hists.mean(axis=0) - all_hists.std(axis=0), all_hists.mean(axis=0) + all_hists.std(axis=0), alpha=0.7)\n",
    "ax.set_ylabel(\"count\", fontsize=14)\n",
    "ax.set_xlabel(\"pixel value\", fontsize=14)\n",
    "ax.set_xlim([-1, 256]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = [cv2.imread(p) for p in tqdm(train_mask_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_proportions = np.array([(img.sum() / 255) / img.size for img in ytrain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(20, 7))\n",
    "fig.tight_layout(h_pad=3)\n",
    "\n",
    "ax1.set_title(\"Distribution proportion salt (all targets)\", fontsize=16)\n",
    "ax1.hist(salt_proportions, bins=50)\n",
    "\n",
    "ax2.set_title(\"Distribution proportion salt (>1% salt coverage)\", fontsize=16)\n",
    "ax2.hist(salt_proportions[salt_proportions > 0.01], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = keras.layers.Input((sample_image.shape[0], sample_image.shape[1], 1), name='image')\n",
    "unet_model = create_unet_model(input_image, batchnorm=True)\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with untrained model\n",
    "\n",
    "Just as a demonstration to see what will happen and what the outputs will be, we'll predict with the untrained model and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = Xtrain[4][:,:,:1]\n",
    "example_target = ytrain[4][:,:,:1]\n",
    "\n",
    "prediction = unet_model.predict(np.array([example_image]) / 255)[0]\n",
    "prediction_mask = np.zeros(shape=prediction[:,:,0].shape)\n",
    "prediction_mask[prediction[:,:,0] > 0.5] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2, ax3, ax4] = plt.subplots(1, 4, figsize=(25, 10))\n",
    "\n",
    "\n",
    "ax1.set_title(\"Input image\", fontsize=14)\n",
    "ax1.imshow(example_image[:,:,0], cmap='seismic', interpolation='bilinear')\n",
    "ax2.set_title(\"True target/mask\", fontsize=14)\n",
    "ax2.imshow(example_target[:,:,0], cmap='gray', interpolation='bilinear')\n",
    "ax3.set_title(\"Raw prediction\", fontsize=14)\n",
    "ax3.imshow(prediction[:,:,0], cmap='gray', interpolation='bilinear')\n",
    "ax4.set_title(\"Prediction mask (threshold 0.5)\", fontsize=14)\n",
    "ax4.imshow(prediction_mask, cmap='gray', interpolation='bilinear');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the model still needs to learn, but at least we have seen the model perform predictions and know the architecture can handle the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
