{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4bebb-576a-4bf9-8609-166d404053bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8eff04-e258-46b5-8b81-96accf537ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from normalizing_flows.src.data_pipeline.dataset import CelebADataset, RandomSubsetDataset\n",
    "from normalizing_flows.src.realnvp import realnvp_flow\n",
    "from normalizing_flows.src.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ec982-8eca-455d-9df3-3c2fea9edb6c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb1abd-99fb-4d19-ba0f-44bc605af209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load CelebA dataset\n",
    "dataset = CelebADataset(\n",
    "    root='../data',\n",
    "    split='train',\n",
    "    download=True,  # if you have trouble downloading the images, download them manually and move the zip file to ../data/celeba/\n",
    "    transform=transform\n",
    ")\n",
    "dataset_subset = RandomSubsetDataset(dataset, subset_size=30000)\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=dataset.collate_fn_skip_errors\n",
    ")\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"Batch shape: {next(iter(dataloader))[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be37cb-f2d9-4dc6-afbb-50fbd08f6e91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_images(images, nrow=8):\n",
    "    \"\"\"Display a grid of images.\"\"\"\n",
    "    images = images.cpu()\n",
    "    images = images * 0.5 + 0.5  # Denormalize\n",
    "    grid = torchvision.utils.make_grid(images, nrow=nrow)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sample_batch = next(iter(dataloader))[0][:32]\n",
    "show_images(sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e842476e-e75d-4dbf-9874-21e1b36c879b",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5f425-6c0a-4516-895b-22eeaaa0a496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from normalizing_flows.src.realnvp import realnvp_flow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2211e71-0236-4006-bcf2-5fa8bf386403",
   "metadata": {
    "tags": []
   },
   "source": [
    "temp = realnvp_flow.ResNet(3, 3, 32)\n",
    "temp(sample_batch).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa7f79e1-df5f-4a01-9ed5-ad228de422d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "temp = realnvp_flow.CouplingBijection2D(3, 64, mask_type='checkerboard', mask_reverse=False)\n",
    "foo = temp.inverse(temp(sample_batch)[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "642d1899-55c4-4e3e-86fd-c2860cc86741",
   "metadata": {
    "tags": []
   },
   "source": [
    "temp = realnvp_flow.BlockBijection2D(3, 32, n_hidden_layers=1)\n",
    "foo = temp.inverse(temp(sample_batch)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a308f-a1bf-48e0-803f-52439352245a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = realnvp_flow.RealNVP(3, 32, n_hidden_layers=1)\n",
    "# foo = temp(sample_batch)[0]\n",
    "foo = temp.inverse(temp(sample_batch)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d5660-8058-487d-a653-8c05cd31b87e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6b15a-5d02-4de7-bf0d-508e4cddb7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_images(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eada76c-8741-4b24-bc91-5963557b3b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43aa684-9713-41a5-b4f4-d96a5ff7c03c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = realnvp_flow.RealNVPFlow(\n",
    "    in_channels=3,  # RGB images\n",
    "    height=32,\n",
    "    width=32,\n",
    "    hidden_channels=64,\n",
    "    n_hidden_layers=2,\n",
    "    n_coupling_layers=4\n",
    ")\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdcf386-cd8b-4748-8d64-fa39d644ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt, factor=0.5, patience=4, threshold=0.001, threshold_mode='abs'\n",
    ")\n",
    "early_stopping = EarlyStopping(patience=8, threshold=0.001)\n",
    "save_dir = 'checkpoints'\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    save_dir=save_dir,\n",
    "    filename='realnvp_{epoch:03d}_{score:.3f}.pt',\n",
    "    save_best_only=True\n",
    ")\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a15090-c7d0-45ba-8d6e-4ef3d8597699",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()\n",
    "for ep in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for i, (x, _) in tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {ep:02d}\"):\n",
    "        x = x.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = -model.log_prob(x).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.detach().cpu().item()\n",
    "\n",
    "    loss_avg = loss_sum / len(dataloader)\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    scheduler.step(loss_avg)\n",
    "    model_checkpoint.save(model, score=loss_avg, epoch=ep)\n",
    "\n",
    "    print(f\"Epoch {ep+1}/{n_epochs}, loss: {loss_avg:.4f}, lr: {lr}\")\n",
    "\n",
    "    if early_stopping(loss_avg):\n",
    "        print(f'EarlyStopping activated. Ending training now.')\n",
    "        break\n",
    "\n",
    "best_path = os.path.join('checkpoints', os.listdir('checkpoints')[-1])\n",
    "print(f\"Loading best model from checkpoint: {best_path}.\")\n",
    "model_checkpoint.load(model, best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512154f-b123-4b42-8183-0c14f617c28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    samples = model.sample(32)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa317247-534c-4a72-826f-2705c64ebf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_latent_space(model, dataloader, device, n_samples=1000):\n",
    "    \"\"\"Visualize the latent space of the model.\"\"\"\n",
    "    model = model.eval()\n",
    "    zs = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(dataloader):\n",
    "            if i * dataloader.batch_size >= n_samples:\n",
    "                break\n",
    "            x = x.to(device)\n",
    "            z, _ = model.forward(x)\n",
    "            zs.append(z.cpu())\n",
    "    \n",
    "    zs = torch.cat(zs, dim=0)[:n_samples]\n",
    "    zs = zs.view(n_samples, -1)  # Flatten\n",
    "    \n",
    "    # Plot first two dimensions\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(zs[:, 0], zs[:, 1], alpha=0.5)\n",
    "    plt.title('Latent Space Visualization')\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c9c163-9f2b-41b2-ad9e-3bbe7202dcee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_latent_space(model, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9e440-9e37-4081-ad44-9cf86fcc7ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92615bd-0369-429b-a21f-57ad577e4eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flows",
   "language": "python",
   "name": "flows"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
