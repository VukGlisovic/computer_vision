{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e060f22a-1cff-4b7b-ae96-c82089e504ee",
   "metadata": {},
   "source": [
    "Notebook for checking individual layers and the complete model to compare inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8e8ba-95a3-4d1b-8d2e-7b3801d0c9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414b46f-8669-45fd-807a-a870ecad2c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from normalizing_flows.src.realnvp.dataset import CelebADataset\n",
    "from normalizing_flows.src.glow.model import layers, blocks, glow_flow\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c94fd-3de9-4f0c-8b55-38862fe7d1d5",
   "metadata": {},
   "source": [
    "# Test individual layers and blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74b128-1994-47bb-8bcd-a8dce06718ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zero_conv2d = layers.Conv2dZeroInit(in_channels=6, out_channels=128)\n",
    "\n",
    "# check if all zero outputs\n",
    "out = zero_conv2d(torch.rand((8, 6, 32, 32)))\n",
    "print(out.shape)\n",
    "(out == 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cfb96-d906-4c2e-ac42-344cff1a0397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inv_conv2d = layers.InvertibleConv2d(in_channels=6)\n",
    "\n",
    "# check if all zero outputs\n",
    "inp = torch.rand((8, 6, 32, 32))\n",
    "out, ldj = inv_conv2d(inp)\n",
    "out = inv_conv2d.inverse(out)\n",
    "print(out.shape, ldj.shape)\n",
    "torch.allclose(inp, out, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ebf4d-11e2-4226-8d76-8e08060b71c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "affine_coupling = layers.AffineCoupling(in_channels=12, hidden_channels=128)\n",
    "\n",
    "inp = torch.rand((8, 12, 32, 32))\n",
    "out, ldj = affine_coupling(inp)\n",
    "out = affine_coupling.inverse(out)\n",
    "print(out.shape, ldj.shape)\n",
    "torch.allclose(inp, out, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970696d-af72-41e1-9414-f276901a2d84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flow_step = blocks.FlowStep(in_channels=12)\n",
    "\n",
    "inp = torch.rand((8, 12, 32, 32))\n",
    "out, ldj = flow_step(inp)\n",
    "out = flow_step.inverse(out)\n",
    "print(out.shape, ldj.shape)\n",
    "torch.allclose(inp, out, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901e00b-2435-4acb-a6e5-ee5064aa52c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf_step = blocks.SqueezeFlowStep(in_channels=12, n_flow_steps=2)\n",
    "\n",
    "inp = torch.rand((8, 12, 32, 32))\n",
    "out, ldj = sf_step(inp)\n",
    "out = sf_step.inverse(out)\n",
    "print(out.shape, ldj.shape)\n",
    "torch.allclose(inp, out, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca4c3b-4aa5-4985-801f-099842a4da8f",
   "metadata": {},
   "source": [
    "# Test realnvp model with input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d256827-3dc5-41ec-99ac-f801db291f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(size=128),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load CelebA dataset\n",
    "ds_train = CelebADataset(\n",
    "    root='../data',\n",
    "    split='train',\n",
    "    download=True,  # if you have trouble downloading the images, download them manually and move the zip file to ../data/celeba/\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=ds_train.collate_fn_skip_errors\n",
    ")\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Train dataset size: {len(ds_train)}\")\n",
    "print(f\"Number of batches: {len(dl_train)}\")\n",
    "sample_batch = next(iter(dl_train))[0][:32]\n",
    "print(f\"Batch shape: {sample_batch.shape}\")\n",
    "\n",
    "\n",
    "def show_images(images, nrow=8):\n",
    "    \"\"\"Display a grid of images.\"\"\"\n",
    "    images = images.cpu()\n",
    "    # images = images * 0.5 + 0.5  # Denormalize\n",
    "    grid = torchvision.utils.make_grid(images, nrow=nrow)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_images(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421bd94-ce1d-4730-af00-00cdd7fad77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = glow_flow.Glow(in_channels=3, size=32, final_size=4, n_flow_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589302bf-8ffd-4dc6-bca2-1adb3f5d2791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out, ldj = model(sample_batch)\n",
    "out = model.inverse(out)\n",
    "print(out.shape, ldj.shape)\n",
    "torch.allclose(sample_batch, out, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b08085-320e-40c4-81ac-1b9b9908ef4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_images(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64e3ce-09e9-4b7b-b3df-e4d6c2476d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74977c19-db64-455e-9e81-6628f69b5194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flows",
   "language": "python",
   "name": "flows"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
