model:
  transformer_layers: 6
  patch_size: 4  # nr pixels height/width
  hidden_size: 64
  num_heads: 4
  mlp_dim: 128

train:
  epochs: 120
  batch_size: 128
  learning_rate: 0.002
