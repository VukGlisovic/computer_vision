{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6625ca9-7c08-4b3c-acd3-0045e0f0bfa8",
   "metadata": {},
   "source": [
    "A notebook for masked language modeling (MLM) and next sentence prediction (NSP).\n",
    "\n",
    "We'll use a BERT model from huggingface: https://huggingface.co/bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41af89f-2b8a-490e-8114-3168ef6bd209",
   "metadata": {},
   "source": [
    "## Masked Language Modeling: predict the missing word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701169b-54c2-46bb-b5bb-c2c9fcd5eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611981a-2475-4f40-80d9-cc4f73434db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert-base-uncased is a relatively small model with 110M parameters and can therefore easily fit into memory\n",
    "model = pipeline('fill-mask', model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0f9b4-6f3d-4732-93ee-752f78a58ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\"Hello world! What a [MASK] day it is!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73baf67f-c810-4f4a-bbc9-811f830967ac",
   "metadata": {},
   "source": [
    "## Next sentence prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216eb8e3-51f0-4085-b7e8-a76fb416032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40570bdd-b38f-4bf4-b49c-01a7e2a369c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BertTokenizer and BertForNextSentencePrediction and move the model to GPU if possible\n",
    "# BertForNextSentencePrediction is a Bert Model with a next sentence prediction (classification) head on top\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba317253-f122-44b9-9678-33438f74f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a positive and negative text pair\n",
    "prompt = \"I will play some tennis today with a friend.\"\n",
    "\n",
    "next_sentence_unlogical = \"Heartbreak pain can be explained through hormonal changes.\"\n",
    "label_unlogical = torch.LongTensor([1]).to(device)\n",
    "\n",
    "next_sentence_logical = \"It's a beautiful day today.\"\n",
    "label_logical = torch.LongTensor([0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ecc334-1198-40e5-a722-ec351df7e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text to tokens and view what they look like\n",
    "tokens_unlogical = tokenizer(prompt, next_sentence_unlogical, return_tensors='pt').to(device)\n",
    "tokens_logical = tokenizer(prompt, next_sentence_logical, return_tensors='pt').to(device)\n",
    "print(tokens_logical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840506b-854d-48bd-935c-9748c2b28a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model inference\n",
    "with torch.no_grad():\n",
    "    output_unlogical = model(**tokens_unlogical, labels=label_unlogical)\n",
    "    output_logical = model(**tokens_logical, labels=label_logical)\n",
    "\n",
    "print(output_unlogical)\n",
    "print(output_logical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64eee3-97c1-4d77-ac97-c1f389873821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_logical(model_output):\n",
    "    logits = model_output.logits\n",
    "    if (logits[0, 0] < logits[0, 1]).item():\n",
    "        print(f\"The next sentence doesn't make sense.\")\n",
    "    else:\n",
    "        print(f\"The next sentence is logical.\")\n",
    "\n",
    "\n",
    "check_if_logical(output_unlogical)\n",
    "check_if_logical(output_logical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3d523-af30-42a1-84f6-40d77d6ab650",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb54830-76ad-46e8-8977-8bff3d85ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf8993-08f4-4db1-97e6-3900340f938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006751c-0a3b-4c9c-9fd2-d653006df8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the text and the question about the text\n",
    "text = \"Joe Carrol is a man who loves to ride the waves whereas Hank, who is thirty years old, prefers to chill in his chair. There will never be any man like Hank.\"\n",
    "question = \"What is Hank his age?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b8d9b-eea2-4777-a60b-dc627272a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inference\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e9ba5-d259-48fb-8abf-694378feceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the start and end index of the text segment that answers the question\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "print(f\"Start/End index answer: {answer_start_index} / {answer_end_index}\")\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "print(f\"{question} -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3dde49-1aa1-4fd6-9851-f378a6a65034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b46f6-b488-4b19-8d1d-210bb47b4d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-pt",
   "language": "python",
   "name": "huggingface-pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
