{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "from tensorflow.keras.metrics import CosineSimilarity\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from siamese_network.data_pipeline import input_dataset\n",
    "from siamese_network.model import embedding\n",
    "from siamese_network.model import siamese_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of paths to anchor and positive images\n",
    "anchor_images = glob(\"../data/left/*\")\n",
    "positive_images = glob(\"../data/right/*\")\n",
    "len(anchor_images), len(positive_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-tuning",
   "metadata": {},
   "source": [
    "# Checkout model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "ds_train, ds_validation = input_dataset.create_triplet_dataset(anchor_images, positive_images, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ds_train.take(1):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements the train and test step to use during training\n",
    "model = siamese_model.SiameseModel()\n",
    "\n",
    "# run one inference to be able to load weights\n",
    "model(x);\n",
    "\n",
    "# load latest weights\n",
    "model.load_weights(glob('../data/results/checkpoints/*.hdf5')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_anchor = model.embedding(x[0])\n",
    "embeddings_positive = model.embedding(x[1])\n",
    "embeddings_negative = model.embedding(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_triplets = 16\n",
    "\n",
    "fig, axes = plt.subplots(n_triplets, 3, figsize=(12, 4.4*n_triplets))\n",
    "fig.tight_layout(w_pad=1)\n",
    "\n",
    "for i in range(n_triplets):\n",
    "    ax_anchor, ax_pos, ax_neg = axes[i]\n",
    "#     ap_similarity = cs(embeddings_anchor[i], embeddings_positive[i]).numpy()\n",
    "#     an_similarity = cs(embeddings_anchor[i], embeddings_negative[i]).numpy()\n",
    "    ap_similarity = siamese_model.l2_distance(embeddings_anchor[i], embeddings_positive[i]).numpy()\n",
    "    an_similarity = siamese_model.l2_distance(embeddings_anchor[i], embeddings_negative[i]).numpy()\n",
    "    ax_anchor.imshow(x[0][i])\n",
    "    ax_pos.set_title(ap_similarity)\n",
    "    ax_pos.imshow(x[1][i])\n",
    "    ax_neg.set_title(an_similarity)\n",
    "    ax_neg.imshow(x[2][i])\n",
    "axes[0, 0].set_title(\"Anchor\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-nothing",
   "metadata": {},
   "source": [
    "# Check how often the correct image is chosen\n",
    "\n",
    "By correct here meaning that the distance between anchor and positive is smaller than the distance between anchor and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "for batch in tqdm(ds_validation):\n",
    "    distances.append(model(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_distances = tf.concat([d[0] for d in distances], axis=0).numpy()\n",
    "an_distances = tf.concat([d[1] for d in distances], axis=0).numpy()\n",
    "\n",
    "print(\"Proportion of triplets where correct image chosen: {:.2f}%\".format((ap_distances < an_distances).mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-pearl",
   "metadata": {},
   "source": [
    "# Create embedding matrix from anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_anchor = input_dataset.create_images_dataset(anchor_images, map_preprocessing_fnc=True)\n",
    "ds_anchor = ds_anchor.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_anchor_path = 'embeddings_anchor.npy'\n",
    "\n",
    "try:\n",
    "    embeddings_anchor = np.load(embeddings_anchor_path, allow_pickle=True)\n",
    "    print(\"Loaded embeddings from '{}'\".format(embeddings_anchor_path))\n",
    "except FileNotFoundError:\n",
    "    print(\"Producing embeddings...\")\n",
    "    embeddings_anchor = model.embedding.predict(ds_anchor, verbose=1)\n",
    "    np.save(embeddings_anchor_path, embeddings_anchor, allow_pickle=True)\n",
    "\n",
    "embeddings_anchor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l2_distances_to_one_image(query_image_path):\n",
    "    \"\"\"Calculates the distances between the query_image and all \n",
    "    the anchor images.\n",
    "    \"\"\"\n",
    "    img = tf.expand_dims(input_dataset.load_and_preprocess_image(query_image_path), axis=0)\n",
    "    query_embedding = model.embedding(img)\n",
    "    distances = list(map(lambda vector: siamese_model.l2_distance(vector, query_embedding), embeddings_anchor))\n",
    "    return tf.concat(distances, axis=0).numpy()\n",
    "\n",
    "\n",
    "def find_top_similar_images(query_image_path, top_k=3):\n",
    "    \"\"\"Finds the top_k anchor images that are most similar to the \n",
    "    query image.\n",
    "    \"\"\"\n",
    "    distances = calculate_l2_distances_to_one_image(query_image_path)\n",
    "    top_indices = np.argpartition(distances, top_k)[:top_k]\n",
    "    top_distances = distances[top_indices]\n",
    "    top_paths = [anchor_images[i] for i in top_indices]\n",
    "    return top_paths, top_distances\n",
    "\n",
    "\n",
    "def visualize_top_similarity_images(query_image_path, query_anchor_path, most_similar_images_paths, most_similar_images_distances):\n",
    "    \"\"\"Plots the query image, the corresponding anchor image and the \n",
    "    top_k most similar images among the anchor images based on the\n",
    "    calculated distances.\n",
    "    \"\"\"\n",
    "    n_cols = 4\n",
    "    top_k = len(most_similar_images_paths)\n",
    "    n_rows = int(np.ceil(top_k / n_cols)) + 1\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, n_rows*4.4))\n",
    "    fig.tight_layout(w_pad=1)\n",
    "\n",
    "    axes[0, 0].imshow(input_dataset.load_and_preprocess_image(query_image_path))\n",
    "    axes[0, 1].imshow(input_dataset.load_and_preprocess_image(query_anchor_path))\n",
    "    axes[0, 1].set_title(\n",
    "        siamese_model.l2_distance(model.embedding(tf.expand_dims(load_and_preprocess_image(query_image_path), axis=0)),\n",
    "                                  model.embedding(tf.expand_dims(load_and_preprocess_image(query_anchor_path), axis=0))).numpy()[0]\n",
    "    )\n",
    "    for i in range(2, n_cols):\n",
    "        axes[0, i].set_visible(False)\n",
    "\n",
    "    sorted_image_paths = [p for _, p in sorted(zip(most_similar_images_distances, most_similar_images_paths))]\n",
    "    sorted_distances = sorted(most_similar_images_distances)\n",
    "\n",
    "    axes_raveled = np.ravel(axes)[n_cols:]\n",
    "    for i in range(top_k):\n",
    "        axes_raveled[i].imshow(load_and_preprocess_image(sorted_image_paths[i]))\n",
    "        axes_raveled[i].set_title(sorted_distances[i])\n",
    "    for i in range(len(axes_raveled) - top_k):\n",
    "        axes_raveled[-i-1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 8\n",
    "i = np.random.randint(len(positive_images))\n",
    "q_image_path = positive_images[i]\n",
    "q_anchor_path = anchor_images[i]\n",
    "closest_image_paths, closest_distances = find_top_similar_images(q_image_path, top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_top_similarity_images(q_image_path, q_anchor_path, closest_image_paths, closest_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-acrylic",
   "metadata": {},
   "source": [
    "# Find proximity of two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rank(idx, query_image_path):\n",
    "    distances_arr = calculate_l2_distances_to_one_image(query_image_path)\n",
    "    rank = (distances_arr < distances_arr[idx]).sum()\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "for i, q_im_path in tqdm(enumerate(positive_images), total=len(positive_images)):\n",
    "    ranks.append(find_rank(i, q_im_path))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "pleasant-firewall",
   "metadata": {},
   "source": [
    "with mp.Pool(max(mp.cpu_count() - 2, 1)) as p:\n",
    "    ranks = p.starmap(find_rank, tqdm(list(enumerate(positive_images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-wealth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-inquiry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
