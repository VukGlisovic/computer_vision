{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from image_captioning.constants import *\n",
    "from image_captioning.data_pipeline import input_dataset, utils\n",
    "from image_captioning.model import text_vectorization, encoder, decoder, checkpoint_manager\n",
    "from image_captioning.scripts.train import create_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-daily",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions, all_imgpaths, imgpath_to_caption = input_dataset.load_annotations()\n",
    "train_featurepaths, train_captions, val_featurepaths, val_captions = input_dataset.split_dataset(all_imgpaths, imgpath_to_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgpaths = list(map(utils.featurepath_to_imgpath, train_featurepaths))\n",
    "val_imgpaths = list(map(utils.featurepath_to_imgpath, val_featurepaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-polymer",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.load_json_file(os.path.join(PROJECT_PATH, 'config.json'))\n",
    "max_length = config['max_text_length']\n",
    "\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text_vectorization.load_text_vectorizer(TOKENIZER_PATH)\n",
    "word_to_index = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionV3 = encoder.create_inception_v3()\n",
    "cnn_encoder, rnn_decoder, optimizer = create_models(config['vocabulary_size'])\n",
    "ckpt_manager = checkpoint_manager.create_checkpoint_manager(cnn_encoder, rnn_decoder, optimizer, restore_latest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-wedding",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = rnn_decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(input_dataset.load_and_preprocess_image(image)[0], 0)\n",
    "    img_tensor_val = inceptionV3(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = cnn_encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = rnn_decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n",
    "        result.append(predicted_word)\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = input_dataset.load_image(image).numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for i in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "        grid_size = max(int(np.ceil(len_result/2)), 2)\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
    "        ax.set_title(result[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(val_imgpaths))\n",
    "image = val_imgpaths[rid]\n",
    "# real_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy())\n",
    "#                          for i in all_captions[rid] if i not in [0]])\n",
    "real_caption = val_captions[rid]\n",
    "\n",
    "result, attention_plot = evaluate(image)\n",
    "evaluate(random_image_path)\n",
    "\n",
    "print('Real Caption:', real_caption)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-privacy",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all captions together having the same image ID.\n",
    "imgpath_to_caption = collections.defaultdict(list)\n",
    "for ann in annotations['annotations']:\n",
    "    caption = f\"<start> {ann['caption']} <end>\"\n",
    "    image_path = os.path.join(image_folder, 'COCO_train2014_{:012d}.jpg'.format(ann['image_id']))\n",
    "    imgpath_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(imgpath_to_caption.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize random image with its captions\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "random_image_path = np.random.choice(image_paths)\n",
    "ax.set_title(\"\\n\".join(imgpath_to_caption[random_image_path]), fontsize=14)\n",
    "ax.imshow(plt.imread(random_image_path));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-memphis",
   "metadata": {},
   "source": [
    "# Data preprocess image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_path = np.random.choice(image_paths)\n",
    "preprocessed_img, _ = input_dataset.load_and_preprocess_image(random_image_path)\n",
    "\n",
    "print(preprocessed_img.shape)\n",
    "plt.imshow((preprocessed_img + 1) / 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-cleanup",
   "metadata": {},
   "source": [
    "# Encode images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "all_imgpaths = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    caption_list = imgpath_to_caption[image_path]\n",
    "    all_captions.extend(caption_list)\n",
    "    all_imgpaths.extend([image_path] * len(caption_list))  # duplicate image path so that every caption has its own image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize inceptionV3 network with imagenet weights\n",
    "inceptionV3 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgpath_to_featurepath(path):\n",
    "    filename = os.path.basename(path)\n",
    "    filename = filename.replace('.jpg', '.npy')\n",
    "    return os.path.join(DATA_DIR, 'train2014_features', filename)  # store features in separate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(DATA_DIR, 'train2014_features'), exist_ok=True)  # directory where to store processed features\n",
    "\n",
    "# get unique image paths that are not processed yet\n",
    "img_paths_processed = [p.replace('_features', '').replace('.npy', '.jpg') for p in glob(os.path.join(image_folder + '_features', '*'))]\n",
    "encode_images_list = sorted(set(all_imgpaths) - set(img_paths_processed))\n",
    "print(f\"Number of images left to process: {len(encode_images_list)}\")\n",
    "\n",
    "if len(encode_images_list) > 0:\n",
    "    # create dataset that returns images and their corresponding filepaths\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_images_list)\n",
    "    image_dataset = image_dataset.map(input_dataset.load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    image_dataset = image_dataset.batch(32)\n",
    "\n",
    "    for batch_imgs, batch_paths in tqdm(image_dataset):\n",
    "\n",
    "        batch_features = inceptionV3(batch_imgs)  # output shape (bs, 8, 8, 2048)\n",
    "        batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))  # output shape (bs, 64, 2048); basically flattens the spatial dimension\n",
    "\n",
    "        for bf, p in zip(batch_features, batch_paths):\n",
    "            img_path = p.numpy().decode(\"utf-8\")\n",
    "            feature_path = imgpath_to_featurepath(img_path)\n",
    "            np.save(feature_path, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-auditor",
   "metadata": {},
   "source": [
    "# Preprocess and tokenize captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50  # max word count for a caption\n",
    "vocabulary_size = 5000  # use the top 5000 words for a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer_path = os.path.join(DATA_DIR, 'experiment/tokenizer.pkl')\n",
    "\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = text_vectorization.load_text_vectorizer(tokenizer_path)\n",
    "else:\n",
    "    tokenizer = text_vectorization.fit_text_vectorizer(all_captions, text_vectorization.standardize_text, max_length, vocabulary_size)\n",
    "    text_vectorization.save_text_vectorizer(tokenizer, tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-buffalo",
   "metadata": {},
   "source": [
    "# Split data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_captions), len(all_imgpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_imgpaths = list(set(all_imgpaths))\n",
    "np.random.shuffle(unique_imgpaths)\n",
    "\n",
    "slice_index = int(len(unique_imgpaths) * 0.8)\n",
    "\n",
    "train_featurepaths = []\n",
    "train_captions = []\n",
    "for imgt in unique_imgpaths[:slice_index]:\n",
    "    \n",
    "    feature_path = imgpath_to_featurepath(imgt)\n",
    "\n",
    "    capt_len = len(imgpath_to_caption[imgt])\n",
    "    train_featurepaths.extend([feature_path] * capt_len)\n",
    "    train_captions.extend(imgpath_to_caption[imgt])\n",
    "\n",
    "val_featurepaths = []\n",
    "val_captions = []\n",
    "for imgv in unique_imgpaths[slice_index:]:\n",
    "    \n",
    "    feature_path = imgpath_to_featurepath(imgv)\n",
    "    \n",
    "    capv_len = len(imgpath_to_caption[imgv])\n",
    "    val_featurepaths.extend([feature_path] * capv_len)\n",
    "    val_captions.extend(imgpath_to_caption[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_featurepaths), len(train_captions), len(val_featurepaths), len(val_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-liability",
   "metadata": {},
   "source": [
    "# TF dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "num_steps = len(train_featurepaths) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8'), allow_pickle=True)\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-member",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_featurepaths, train_captions))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda path, text: (path, tokenizer(text)))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda path, text: tf.numpy_function(map_func, [path, text], [tf.float32, tf.int64]), \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset \\\n",
    "    .shuffle(BUFFER_SIZE) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "intermediate-arthur",
   "metadata": {},
   "source": [
    "for x, y in dataset.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-pension",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder = encoder.CNN_Encoder(embedding_dim)\n",
    "rnn_decoder = decoder.RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-agenda",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(DATA_DIR, 'experiment/checkpoints/train')\n",
    "ckpt = tf.train.Checkpoint(encoder=cnn_encoder,\n",
    "                           decoder=rnn_decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-shirt",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings for words to indices and indicies to words.\n",
    "word_to_index = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = rnn_decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = cnn_encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = rnn_decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = cnn_encoder.trainable_variables + rnn_decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "ax.set_title(\"Loss Curve\", fontsize=20)\n",
    "ax.plot(list(np.random.randint(0, 10, size=20)), lw=2.5, alpha=0.8)\n",
    "ax.grid(ls='--')\n",
    "ax.set_xlim(0)\n",
    "ax.set_ylim(0)\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=16)\n",
    "\n",
    "fig.savefig('test.png', format='png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-berlin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-belief",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
