{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from image_captioning.constants import *\n",
    "from image_captioning.data_pipeline import input_dataset, utils\n",
    "from image_captioning.model import text_vectorization, encoder, decoder, checkpoint_manager\n",
    "from image_captioning.scripts.train import create_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-representation",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions, all_imgpaths, imgpath_to_caption = input_dataset.load_annotations()\n",
    "train_featurepaths, train_captions, val_featurepaths, val_captions = input_dataset.split_dataset(all_imgpaths, imgpath_to_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgpaths = list(map(utils.featurepath_to_imgpath, train_featurepaths))\n",
    "val_imgpaths = list(map(utils.featurepath_to_imgpath, val_featurepaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-stuff",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.load_json_file(os.path.join(PROJECT_PATH, 'config.json'))\n",
    "max_length = config['max_text_length']\n",
    "\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text_vectorization.load_text_vectorizer(TOKENIZER_PATH)\n",
    "word_to_index = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionV3 = encoder.create_inception_v3()\n",
    "cnn_encoder, rnn_decoder, optimizer = create_models(config['vocabulary_size'])\n",
    "ckpt_manager = checkpoint_manager.create_checkpoint_manager(cnn_encoder, rnn_decoder, optimizer, restore_latest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-blank",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = rnn_decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(input_dataset.load_and_preprocess_image(image)[0], 0)\n",
    "    img_tensor_val = inceptionV3(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = cnn_encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = rnn_decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n",
    "        result.append(predicted_word)\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, sentence, attention_weights):\n",
    "    temp_image = input_dataset.load_image(image).numpy()\n",
    "    ratio = temp_image.shape[0] / temp_image.shape[1]\n",
    "    \n",
    "    n_words = len(sentence)\n",
    "    n_cols = 4\n",
    "    n_rows = int(np.ceil(n_words / n_cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows * ratio))\n",
    "    axes_raveled = axes.ravel()\n",
    "    \n",
    "    for i in range(n_words):\n",
    "        ax = axes_raveled[i]\n",
    "        temp_att = np.resize(attention_weights[i], (8, 8))\n",
    "        ax.set_title(sentence[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "    for ax in axes_raveled[n_words:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(val_imgpaths))\n",
    "random_image_path = val_imgpaths[rid]\n",
    "real_caption = val_captions[rid]\n",
    "\n",
    "result, attention_plot = evaluate(random_image_path)\n",
    "\n",
    "print('Real Caption:', real_caption)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(random_image_path, result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-purse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-version",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
