{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from image_captioning.constants import DATA_DIR\n",
    "from image_captioning.data_pipeline import input_dataset\n",
    "from image_captioning.model import text_vectorization, encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-transport",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "This can take some time as it's ~13GB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download caption annotation files\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    annotation_zip = tf.keras.utils.get_file(\n",
    "        fname='captions.zip',\n",
    "        origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "        cache_subdir=DATA_DIR,\n",
    "        extract=True\n",
    "    )\n",
    "    os.remove(annotation_zip)\n",
    "\n",
    "annotation_file = os.path.join(DATA_DIR, 'annotations/captions_train2014.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download image files\n",
    "image_folder = os.path.join(DATA_DIR, 'train2014')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    image_zip = tf.keras.utils.get_file(\n",
    "        fname='train2014.zip',\n",
    "        origin='http://images.cocodataset.org/zips/train2014.zip',\n",
    "        cache_subdir=image_folder,\n",
    "        extract=True\n",
    "    )\n",
    "    os.remove(image_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-boring",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all captions together having the same image ID.\n",
    "imgpath_to_caption = collections.defaultdict(list)\n",
    "for ann in annotations['annotations']:\n",
    "    caption = f\"<start> {ann['caption']} <end>\"\n",
    "    image_path = os.path.join(image_folder, 'COCO_train2014_{:012d}.jpg'.format(ann['image_id']))\n",
    "    imgpath_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(imgpath_to_caption.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize random image with its captions\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "random_image_path = np.random.choice(image_paths)\n",
    "ax.set_title(\"\\n\".join(imgpath_to_caption[random_image_path]), fontsize=14)\n",
    "ax.imshow(plt.imread(random_image_path));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-august",
   "metadata": {},
   "source": [
    "# Data preprocess image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_path = np.random.choice(image_paths)\n",
    "preprocessed_img, _ = input_dataset.load_and_preprocess_image(random_image_path)\n",
    "\n",
    "print(preprocessed_img.shape)\n",
    "plt.imshow((preprocessed_img + 1) / 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-spider",
   "metadata": {},
   "source": [
    "# Encode images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "all_imgpaths = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    caption_list = imgpath_to_caption[image_path]\n",
    "    all_captions.extend(caption_list)\n",
    "    all_imgpaths.extend([image_path] * len(caption_list))  # duplicate image path so that every caption has its own image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize inceptionV3 network with imagenet weights\n",
    "inceptionV3 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgpath_to_featurepath(path):\n",
    "    filename = os.path.basename(path)\n",
    "    filename = filename.replace('.jpg', '.npy')\n",
    "    return os.path.join(DATA_DIR, 'train2014_features', filename)  # store features in separate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(DATA_DIR, 'train2014_features'), exist_ok=True)  # directory where to store processed features\n",
    "\n",
    "# get unique image paths that are not processed yet\n",
    "img_paths_processed = [p.replace('_features', '').replace('.npy', '.jpg') for p in glob(os.path.join(image_folder + '_features', '*'))]\n",
    "encode_images_list = sorted(set(all_imgpaths) - set(img_paths_processed))\n",
    "print(f\"Number of images left to process: {len(encode_images_list)}\")\n",
    "\n",
    "if len(encode_images_list) > 0:\n",
    "    # create dataset that returns images and their corresponding filepaths\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_images_list)\n",
    "    image_dataset = image_dataset.map(input_dataset.load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    image_dataset = image_dataset.batch(32)\n",
    "\n",
    "    for batch_imgs, batch_paths in tqdm(image_dataset):\n",
    "\n",
    "        batch_features = inceptionV3(batch_imgs)  # output shape (bs, 8, 8, 2048)\n",
    "        batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))  # output shape (bs, 64, 2048); basically flattens the spatial dimension\n",
    "\n",
    "        for bf, p in zip(batch_features, batch_paths):\n",
    "            img_path = p.numpy().decode(\"utf-8\")\n",
    "            feature_path = imgpath_to_featurepath(img_path)\n",
    "            np.save(feature_path, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-pontiac",
   "metadata": {},
   "source": [
    "# Preprocess and tokenize captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50  # max word count for a caption\n",
    "vocabulary_size = 5000  # use the top 5000 words for a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer_path = os.path.join(DATA_DIR, 'experiment/tokenizer.pkl')\n",
    "\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = text_vectorization.load_text_vectorizer(tokenizer_path)\n",
    "else:\n",
    "    tokenizer = text_vectorization.fit_text_vectorizer(all_captions, text_vectorization.standardize_text, max_length, vocabulary_size)\n",
    "    text_vectorization.save_text_vectorizer(tokenizer, tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-symphony",
   "metadata": {},
   "source": [
    "# Split data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_captions), len(all_imgpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_imgpaths = list(set(all_imgpaths))\n",
    "np.random.shuffle(unique_imgpaths)\n",
    "\n",
    "slice_index = int(len(unique_imgpaths) * 0.8)\n",
    "\n",
    "train_featurepaths = []\n",
    "train_captions = []\n",
    "for imgt in unique_imgpaths[:slice_index]:\n",
    "    \n",
    "    feature_path = imgpath_to_featurepath(imgt)\n",
    "\n",
    "    capt_len = len(imgpath_to_caption[imgt])\n",
    "    train_featurepaths.extend([feature_path] * capt_len)\n",
    "    train_captions.extend(imgpath_to_caption[imgt])\n",
    "\n",
    "val_featurepaths = []\n",
    "val_captions = []\n",
    "for imgv in unique_imgpaths[slice_index:]:\n",
    "    \n",
    "    feature_path = imgpath_to_featurepath(imgv)\n",
    "    \n",
    "    capv_len = len(imgpath_to_caption[imgv])\n",
    "    val_featurepaths.extend([feature_path] * capv_len)\n",
    "    val_captions.extend(imgpath_to_caption[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_featurepaths), len(train_captions), len(val_featurepaths), len(val_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-contemporary",
   "metadata": {},
   "source": [
    "# TF dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "num_steps = len(train_featurepaths) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8'), allow_pickle=True)\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-fusion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_featurepaths, train_captions))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda path, text: (path, tokenizer(text)))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda path, text: tf.numpy_function(map_func, [path, text], [tf.float32, tf.int64]), \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset \\\n",
    "    .shuffle(BUFFER_SIZE) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "gross-registration",
   "metadata": {},
   "source": [
    "for x, y in dataset.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-ferry",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder = encoder.CNN_Encoder(embedding_dim)\n",
    "rnn_decoder = decoder.RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-trading",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(DATA_DIR, 'experiment/checkpoints/train')\n",
    "ckpt = tf.train.Checkpoint(encoder=cnn_encoder,\n",
    "                           decoder=rnn_decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-ethnic",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings for words to indices and indicies to words.\n",
    "word_to_index = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = rnn_decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = cnn_encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = rnn_decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = cnn_encoder.trainable_variables + rnn_decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-trace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-private",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
