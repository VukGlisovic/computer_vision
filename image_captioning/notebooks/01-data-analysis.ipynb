{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from image_captioning.constants import DATA_DIR\n",
    "from image_captioning.data_pipeline import input_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-apparatus",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "This can take some time as it's ~13GB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download caption annotation files\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    annotation_zip = tf.keras.utils.get_file(\n",
    "        fname='captions.zip',\n",
    "        origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "        cache_subdir=DATA_DIR,\n",
    "        extract=True\n",
    "    )\n",
    "    os.remove(annotation_zip)\n",
    "\n",
    "annotation_file = os.path.join(DATA_DIR, 'annotations/captions_train2014.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download image files\n",
    "image_folder = os.path.join(DATA_DIR, 'train2014')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    image_zip = tf.keras.utils.get_file(\n",
    "        fname='train2014.zip',\n",
    "        origin='http://images.cocodataset.org/zips/train2014.zip',\n",
    "        cache_subdir=image_folder,\n",
    "        extract=True\n",
    "    )\n",
    "    os.remove(image_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-villa",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all captions together having the same image ID.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    image_path = os.path.join(image_folder, 'COCO_train2014_{:012d}.jpg'.format(val['image_id']))\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "random_image_path = np.random.choice(image_paths)\n",
    "ax.set_title(\"\\n\".join(image_path_to_caption[random_image_path]), fontsize=14)\n",
    "ax.imshow(plt.imread(random_image_path));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-ticket",
   "metadata": {},
   "source": [
    "# Data preprocess image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_path = np.random.choice(image_paths)\n",
    "preprocessed_img, _ = input_dataset.load_and_preprocess_image(random_image_path)\n",
    "\n",
    "print(preprocessed_img.shape)\n",
    "plt.imshow((preprocessed_img + 1) / 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-ottawa",
   "metadata": {},
   "source": [
    "# Encode images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    train_captions.extend(caption_list)\n",
    "    img_name_vector.extend([image_path] * len(caption_list))  # duplicate image path so that every caption has its own image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize inceptionV3 network with imagenet weights\n",
    "inceptionV3 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(DATA_DIR, 'train2014_features'), exist_ok=True)  # directory where to store processed features\n",
    "\n",
    "# get unique image paths that are not processed yet\n",
    "img_paths_processed = [p.replace('_features', '').replace('.npy', '.jpg') for p in glob(os.path.join(image_folder + '_features', '*'))]\n",
    "encode_train = sorted(set(img_name_vector) - set(img_paths_processed))\n",
    "print(f\"Number of images left to process: {len(encode_train)}\")\n",
    "\n",
    "# create dataset that returns images and their corresponding filepaths\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(input_dataset.load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "image_dataset = image_dataset.batch(32)\n",
    "\n",
    "for batch_imgs, batch_paths in tqdm(image_dataset):\n",
    "    \n",
    "    batch_features = inceptionV3(batch_imgs)  # output shape (bs, 8, 8, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))  # output shape (bs, 64, 2048); basically flattens the spatial dimension\n",
    "\n",
    "    for bf, p in zip(batch_features, batch_paths):\n",
    "        img_path = p.numpy().decode(\"utf-8\")\n",
    "        filename = img_path.split('/')[-1].split('.')[0]\n",
    "        output_path = os.path.join(DATA_DIR, 'train2014_features', filename)  # store features in separate directory\n",
    "        np.save(output_path, bf.numpy())  # will append .npy to output filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-queue",
   "metadata": {},
   "source": [
    "# Preprocess and tokenize captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(text):\n",
    "    \"\"\"In order to preserve the '<>' tokens for <start> and <end>,\n",
    "    we will override the default standardization of TextVectorization.\n",
    "    \n",
    "    Args:\n",
    "        text (Union[str, tf.Tensor]):\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor\n",
    "    \"\"\"\n",
    "    text = tf.strings.lower(text)\n",
    "    return tf.strings.regex_replace(text, r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n",
    "\n",
    "max_length = 50  # max word count for a caption\n",
    "vocabulary_size = 5000  # use the top 5000 words for a vocabulary\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, \n",
    "    standardize=standardize_text, \n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "\n",
    "# Learn the vocabulary from the caption data.\n",
    "tokenizer.adapt(caption_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "cap_vector = caption_dataset.map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings for words to indices and indicies to words.\n",
    "word_to_index = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-builder",
   "metadata": {},
   "source": [
    "# Split data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys) * 0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-detection",
   "metadata": {},
   "source": [
    "# TF dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(\n",
    "    lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int64]), \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset \\\n",
    "    .shuffle(BUFFER_SIZE) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-disco",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-assets",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(DATA_DIR, 'experiment/checkpoints/train')\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-jamaica",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-offense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-rouge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-honor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
