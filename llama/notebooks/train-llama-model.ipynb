{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4209d50-91f0-432d-aab8-501185351398",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf9ad7-6aba-4d85-ace4-43cfebf46733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "# PyTorch for implementing LLM (No GPU)\n",
    "import torch\n",
    "\n",
    "# Neural network modules and functions from PyTorch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# NumPy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for plotting Loss etc.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Time module for tracking execution time\n",
    "import time\n",
    "\n",
    "# Pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# urllib for handling URL requests (Downloading Dataset)\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "from llama.model.custom_layers import *\n",
    "from llama.model.custom_blocks import *\n",
    "from llama.model.model import Llama\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38ff6c-4b30-4718-bfbb-5d9b99f414f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration object for model parameters\n",
    "MASTER_CONFIG = {\n",
    "    'vocab_size': -1,         # TBD\n",
    "    'batch_size': 32,          # Number of batches to be processed at each random split\n",
    "    'context_window': 16,     # Number of characters in each input (x) and target (y) sequence of each batch\n",
    "    'd_model': 128,           # Dimension of linear layers (128)\n",
    "    'epochs': 1000,          # Number of training epochs\n",
    "    'log_interval': 10,      # Log information every 10 batches during training\n",
    "    'batch_size': 32,        # Increase batch size to 32\n",
    "    'n_heads': 8,            # number of attention heads\n",
    "    'n_layers': 4,           # Set the number of layers to 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16571b-4c0e-41eb-9434-7d7f90be7629",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a912097-a9e8-4530-a540-b230641db762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL of the raw text file on GitHub\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "# The file name for local storage\n",
    "file_name = \"tinyshakespeare.txt\"\n",
    "\n",
    "# Execute the download\n",
    "urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4a159-588f-44ff-b929-181c6c440724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the dataset\n",
    "lines = open(\"tinyshakespeare.txt\", 'r').read()\n",
    "\n",
    "# Create a sorted list of unique characters in the dataset\n",
    "character_counts = Counter(lines)\n",
    "vocab = sorted(list(set(lines)))\n",
    "\n",
    "# Display the first 10 characters in the vocabulary list\n",
    "print('Printing the first 10 characters of the vocab list:', vocab[:10])\n",
    "\n",
    "# Output the total number of characters in our dataset (Vocabulary Size)\n",
    "print('Total number of characters in our dataset (Vocabulary Size):', len(vocab))\n",
    "MASTER_CONFIG['vocab_size'] = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6efab58-c0cc-40d4-8a02-892aa2c4803a",
   "metadata": {},
   "source": [
    "# Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfb09b-4e3e-47a3-a4a6-0d2adf7e6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "\n",
    "# Mapping integers to characters (itos)\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "# Mapping characters to integers (stoi)\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "\n",
    "# Encode function: Converts a string to a list of integers using the mapping stoi\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "# Decode function: Converts a list of integers back to a string using the mapping itos\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Example: Encode the string \"hello\" and then decode the result\n",
    "decode(encode(\"morning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a863a6-3ce0-4825-a688-d920934f29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into a torch tensor with specified data type (dtype)\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "\n",
    "# Display the shape of the resulting tensor\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43a47a-2d4d-40ad-a262-e4fd009b7a2f",
   "metadata": {},
   "source": [
    "# Create dataloading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530ddb7-27dc-4df6-b090-cb03c1f43d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get batches for training, validation, or testing\n",
    "def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):\n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    # Determine which split to use\n",
    "    batch_data = train\n",
    "    if split == 'val':\n",
    "        batch_data = val\n",
    "    if split == 'test':\n",
    "        batch_data = test\n",
    "\n",
    "    # Pick random starting points within the data\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "\n",
    "    # Create input sequences (x) and corresponding target sequences (y)\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57237b9-0013-436c-b7df-784ffaad3325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain batches for training using the specified batch size and context window\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Decode the sequences to obtain the corresponding text representations\n",
    "decoded_samples = [(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]\n",
    "\n",
    "# Print the random sample\n",
    "print(decoded_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd013e-4c05-4d1e-a617-e918b695d545",
   "metadata": {},
   "source": [
    "# Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c8ec0-121f-4f17-af7f-75258d26e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Don't compute gradients for this function\n",
    "def evaluate_loss(model, config=MASTER_CONFIG):\n",
    "    # Placeholder for the evaluation results\n",
    "    out = {}\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate through training and validation splits\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        # Placeholder for individual losses\n",
    "        losses = []\n",
    "\n",
    "        # Generate 10 batches for evaluation\n",
    "        for _ in range(10):\n",
    "            # Get input sequences (xb) and target sequences (yb)\n",
    "            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])\n",
    "            \n",
    "            # Perform model inference and calculate the loss\n",
    "            _, loss = model(xb, yb)\n",
    "            \n",
    "            # Append the loss to the list\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Calculate the mean loss for the split and store it in the output dictionary\n",
    "        out[split] = np.mean(losses)\n",
    "    \n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf91e8-8b53-4579-b560-1168e1748f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training\n",
    "def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):\n",
    "    # Placeholder for storing losses\n",
    "    losses = []\n",
    "    \n",
    "    # Start tracking time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through epochs\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Obtain batches for training\n",
    "        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "\n",
    "        # Forward pass through the model to calculate logits and loss\n",
    "        logits, loss = model(xs, targets=ys)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # If a learning rate scheduler is provided, adjust the learning rate\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Log progress every specified interval\n",
    "        if epoch % config['log_interval'] == 0:\n",
    "            # Calculate batch time\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate loss on validation set\n",
    "            x = evaluate_loss(model)\n",
    "            \n",
    "            # Store the validation loss\n",
    "            losses += [x]\n",
    "            \n",
    "            # Print progress logs if specified\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}\")\n",
    "                \n",
    "            # Reset the timer\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Print learning rate if a scheduler is provided\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())\n",
    "\n",
    "    # Print the final validation loss\n",
    "    print(\"Validation loss: \", losses[-1]['val'])\n",
    "    \n",
    "    # Plot the training and validation loss curves\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d097f-bec2-4697-8655-a913c11fdd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate function for text generation using the trained model\n",
    "def generate(model, config=MASTER_CONFIG, max_new_tokens=30):\n",
    "    idx = torch.zeros(5, 1).long()\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Call the model\n",
    "        logits = model(idx[:, -config['context_window']:])\n",
    "        last_time_step_logits = logits[\n",
    "            :, -1, :\n",
    "        ]  # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)  # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )  # sample from the distribution to get the next token\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence\n",
    "    return [decode(x) for x in idx.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205446e-fc34-43aa-9b4d-19f05ee1a6a9",
   "metadata": {},
   "source": [
    "# Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7ec6c-6798-4bf5-af36-904f6e45868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = Llama(MASTER_CONFIG)\n",
    "llama = llama.to(device)\n",
    "\n",
    "# Print total number of parameters in the model\n",
    "print(\"model params:\", sum([p.numel() for p in llama.parameters()]))\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = llama(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(llama.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba435fc-bf69-41a4-ba8c-287a8118603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(llama, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763676b5-e0d7-498f-be8d-f417470eaa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the trained LLM (llama) with a maximum of 500 tokens\n",
    "generated_text = generate(llama, MASTER_CONFIG, 500)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cac7b0-9065-40fc-ba29-7a195437b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test set performance\n",
    "# Get batches from the test set\n",
    "xs, ys = get_batches(dataset, 'test', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Pass the test data through the LLaMA model\n",
    "logits, loss = llama(xs, ys)\n",
    "\n",
    "# Print the loss on the test set\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9216d-20df-41aa-a4ae-225a0ae944d5",
   "metadata": {},
   "source": [
    "# Train for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d00bc-018a-4cb6-9597-cd42bc321b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the number of epochs in the configuration\n",
    "MASTER_CONFIG.update({\n",
    "    'epochs': 10000,\n",
    "})\n",
    "# Train the LLaMA model for the specified number of epochs\n",
    "train(llama, optimizer, scheduler=None, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7e48e-268b-4987-bc18-6c2cb1942b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model again, scheduler for better optimization.\n",
    "# TODO: add scheduler\n",
    "train(llama, optimizer, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87923fd-a73b-4fe3-80a4-a7b4fad05618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update configuration\n",
    "MASTER_CONFIG.update({\n",
    "    \"epochs\": 1000\n",
    "})\n",
    "\n",
    "# Create Llama model with Cosine Annealing learning schedule\n",
    "llama_with_cosine = Llama(MASTER_CONFIG)\n",
    "\n",
    "# Define Adam optimizer with specific hyperparameters\n",
    "llama_optimizer = torch.optim.Adam(\n",
    "    llama.parameters(),\n",
    "    betas=(.9, .95),\n",
    "    weight_decay=.1,\n",
    "    eps=1e-9,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Define Cosine Annealing learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(llama_optimizer, 300, eta_min=1e-5)\n",
    "\n",
    "# Train the Llama model with the specified optimizer and scheduler\n",
    "train(llama_with_cosine, llama_optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07fc681d-7ee1-4cea-90f9-b447cf8a69e3",
   "metadata": {},
   "source": [
    "# Save the entire model\n",
    "torch.save(llama, 'llama_model.pth')\n",
    "\n",
    "# If you want to save only the model parameters\n",
    "torch.save(llama.state_dict(), 'llama_model_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc59642-9c49-4e40-a692-494178ac5dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44776c9-52fc-40b6-b6dd-7a0fd59e72b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
