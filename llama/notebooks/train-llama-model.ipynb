{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4209d50-91f0-432d-aab8-501185351398",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf9ad7-6aba-4d85-ace4-43cfebf46733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from llama.data_pipeline import tiny_shakespeare\n",
    "from llama.data_pipeline import dataloader\n",
    "from llama.model.tokenizer import CharacterTokenizer\n",
    "from llama.model.custom_layers import *\n",
    "from llama.model.custom_blocks import *\n",
    "from llama.model.model import Llama\n",
    "from llama.constants import *\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38ff6c-4b30-4718-bfbb-5d9b99f414f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration object for model parameters\n",
    "CONFIG = {\n",
    "    'vocab_size': -1,         # TBD based on dataset\n",
    "    'batch_size': 64,          # Number of batches to be processed at each random split\n",
    "    'context_window': 16,     # Number of characters in each input (x) and target (y) sequence of each batch\n",
    "    'd_model': 128,           # Dimension of linear layers (128)\n",
    "    'epochs': 3,          # Number of training epochs\n",
    "    'log_interval': 10,      # Log information every 10 batches during training\n",
    "    'batch_size': 32,        # Increase batch size to 32\n",
    "    'n_heads': 8,            # number of attention heads\n",
    "    'n_layers': 4,           # Set the number of layers to 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16571b-4c0e-41eb-9434-7d7f90be7629",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700e4d2-4e49-4e4d-9771-02b00bde4439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stores the tiny shakespeare dataset to disk\n",
    "data_path = tiny_shakespeare.download_tiny_shakespeare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4a159-588f-44ff-b929-181c6c440724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the dataset\n",
    "with open(data_path, 'r') as f:\n",
    "    lines = f.read()\n",
    "\n",
    "# Create a sorted list of unique characters in the dataset\n",
    "character_counts = Counter(lines)\n",
    "vocab = sorted(list(set(lines)))\n",
    "print(character_counts)\n",
    "\n",
    "# update the vocabulary size in the configuration\n",
    "CONFIG['vocab_size'] = len(vocab)\n",
    "\n",
    "# Output the total number of characters in our dataset (Vocabulary Size)\n",
    "print(f'Total number of characters in our dataset (Vocabulary Size): {CONFIG[\"vocab_size\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6efab58-c0cc-40d4-8a02-892aa2c4803a",
   "metadata": {},
   "source": [
    "# Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba11f4-ca55-4eb6-9de3-ecd281292167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = CharacterTokenizer(vocab)\n",
    "\n",
    "# check encode and decode functions\n",
    "tokenizer.decode(tokenizer.encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea2ca9-ce7b-42dc-8077-2476a323ed32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the text data\n",
    "train_split = lines[:int(0.8 * len(lines))]\n",
    "val_split = lines[int(0.8 * len(lines)): int(0.9 * len(lines))]\n",
    "test_split = lines[int(0.9 * len(lines)):]\n",
    "\n",
    "# create a dataset for each split\n",
    "train_dataset = dataloader.TextDataset(train_split, tokenizer, CONFIG['context_window'], device)\n",
    "val_dataset = dataloader.TextDataset(val_split, tokenizer, CONFIG['context_window'], device)\n",
    "test_dataset = dataloader.TextDataset(test_split, tokenizer, CONFIG['context_window'], device)\n",
    "\n",
    "# create a dataloader for each split\n",
    "bs = CONFIG['batch_size']\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "print(f\"Steps train: {len(train_dataloader)}, val: {len(val_dataloader)}, test: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd013e-4c05-4d1e-a617-e918b695d545",
   "metadata": {},
   "source": [
    "# Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cb10c-3a3f-42a6-bea3-2ac6abd20401",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_loss(model, dataloader):\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    for x, y in dataloader:\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def train(model, optimizer, n_epochs, scheduler=None):\n",
    "    # set model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Placeholder for storing losses\n",
    "    metrics = {'train': [], 'val': []}\n",
    "    if scheduler:\n",
    "        metrics['lr'] = []\n",
    "\n",
    "    # Iterate through epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        train_losses = []\n",
    "        for x, y in (pbar := tqdm(train_dataloader)):\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the model to calculate logits and loss\n",
    "            logits, loss = model(x, targets=y)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train = loss.item()\n",
    "            train_losses.append(loss_train)\n",
    "            pbar.set_description(f\"Ep {epoch+1}/{n_epochs} | Train loss {loss_train:.3f}\")\n",
    "            \n",
    "        metrics['train'].append(np.mean(train_losses))\n",
    "\n",
    "        # adjust the learning rate if there is a lr scheduler\n",
    "        if scheduler:\n",
    "            metrics['lr'].append(scheduler.get_last_lr()[0])\n",
    "            scheduler.step()\n",
    "\n",
    "        # evaluate loss on the validation set\n",
    "        loss_val = evaluate_loss(model, val_dataloader)\n",
    "        metrics['val'].append(loss_val)\n",
    "        print(f\"Ep {epoch+1}/{n_epochs} | Train loss {metrics['train'][-1]:.3f} | Val loss {loss_val:.3f}\")\n",
    "    \n",
    "    # Plot the training and validation loss curves\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205446e-fc34-43aa-9b4d-19f05ee1a6a9",
   "metadata": {},
   "source": [
    "# Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520fa723-8a15-48df-a29b-b00e66acefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Llama model\n",
    "llama = Llama(CONFIG)\n",
    "llama = llama.to(device)\n",
    "print(f\"Model params: {sum([p.numel() for p in llama.parameters()]):,}\")\n",
    "\n",
    "# create the corresponding optimizer\n",
    "optimizer = torch.optim.Adam(llama.parameters(), lr=1e-3)\n",
    "\n",
    "# create a step learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253ef0a-238f-48c4-a708-ee568bbbcb84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_losses = train(llama, optimizer, CONFIG['epochs'], lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2927908-faa5-4a64-8a4b-ce4fbbe64f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "df_losses[['train', 'val']].plot(ax=ax1)\n",
    "df_losses[['lr']].plot(ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85678d8-bdf8-4adc-ad1c-d55e5714d371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# check loss on test split\n",
    "evaluate_loss(llama, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c861d3-d62e-44ff-b2be-fd9a661680fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate text using the trained LLM (llama) with a maximum of 500 tokens\n",
    "generated_text = llama.generate(device, tokenizer, 500)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef857b-3831-4938-ae53-5f8d99d8e649",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7dad92-24f7-4cd7-89d1-a495108da563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(EXPERIMENTS_DIR, exist_ok=True)\n",
    "\n",
    "# save the entire model\n",
    "torch.save(llama, os.path.join(EXPERIMENTS_DIR, 'llama.pth'))\n",
    "\n",
    "# save only the model parameters\n",
    "# torch.save(llama.state_dict(), os.path.join(EXPERIMENTS_DIR, 'llama_model_parameters.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b364360-c4f6-4d22-897b-79dc862bc00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check loaded model\n",
    "llama_loaded = torch.load(os.path.join(EXPERIMENTS_DIR, 'llama.pth'))\n",
    "\n",
    "print(llama_loaded.generate(device, tokenizer, max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c1b95-0498-4cb2-93a9-dfbddb68bbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44776c9-52fc-40b6-b6dd-7a0fd59e72b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
