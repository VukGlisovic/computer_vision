{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4209d50-91f0-432d-aab8-501185351398",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf9ad7-6aba-4d85-ace4-43cfebf46733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import SentencePieceBPETokenizer, Tokenizer\n",
    "\n",
    "from llama.data_pipeline import gutenberg_data, dataset\n",
    "from llama.model.tokenizer import CharacterTokenizer\n",
    "from llama.model.custom_layers import *\n",
    "from llama.model.custom_blocks import *\n",
    "from llama.model import model, training\n",
    "from llama.constants import *\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38ff6c-4b30-4718-bfbb-5d9b99f414f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration object for model parameters\n",
    "CONFIG = {\n",
    "    'vocab_size': -1,        # TBD based on dataset and tokenizer\n",
    "    'batch_size': 128,       # Number of batches to be processed at each random split\n",
    "    'epochs': 5,             # Number of training epochs\n",
    "    'context_window': 32,    # Number of characters in each input (x) and target (y) sequence of each batch\n",
    "    'd_model': 128,          # Dimension of linear layers (128)\n",
    "    'n_heads': 8,            # number of attention heads\n",
    "    'n_layers': 4,           # Set the number of layers to 4\n",
    "}\n",
    "\n",
    "experiment_dir = os.path.join(EXPERIMENTS_DIR, 'sentencepiece_llama_gutenberg')\n",
    "os.makedirs(experiment_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16571b-4c0e-41eb-9434-7d7f90be7629",
   "metadata": {},
   "source": [
    "# Load dataset and fit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a196a5a-d6bc-488b-ab0d-fc1f37055fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = gutenberg_data.how_to_get_gutenberg_subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd66caa-3619-4f7e-b1bd-c04a862c99fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gb_data, df_metadata = gutenberg_data.load_gutenberg_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568540c5-2004-47d6-b001-365eaf7b9b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_text = \"\".join(list(gb_data.values()))\n",
    "print(f\"Number of characters in all text: {len(all_text):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c5670-c936-4c76-b819-c802007b0c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the text data and take a subset (cause the data is quite big)\n",
    "train_split = [gb_data[fn] for fn in df_metadata.groupby('author')['id'].sample(1, random_state=42)]\n",
    "val_split = [gb_data[fn] for fn in df_metadata['id'].sample(20, random_state=42)]\n",
    "test_split = [gb_data[fn] for fn in df_metadata['id'].sample(20, random_state=43)]\n",
    "print(f\"Number of characters in all train: {sum([len(text) for text in train_split]):,}\")\n",
    "print(f\"Number of characters in all val/test: {sum([len(text) for text in val_split]):,} / {sum([len(text) for text in test_split]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3b65b-7473-46bb-9dea-4f78246a20db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer_path = os.path.join(experiment_dir, 'sentencepiece_tokenizer.json')\n",
    "\n",
    "if os.path.isfile(tokenizer_path):\n",
    "    # load the tokenizer from disk\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "    print(f\"Loaded tokenizer from disk ('{tokenizer_path}')\")\n",
    "else:\n",
    "    # train the tokenizer from scratch with a subset of the text data for speed\n",
    "    unk_token = '<unk>'\n",
    "    tokenizer = SentencePieceBPETokenizer(unk_token=unk_token)\n",
    "    tokenizer.train_from_iterator(\n",
    "        train_split,\n",
    "        vocab_size=3000,\n",
    "        min_frequency=10,\n",
    "        show_progress=True,\n",
    "        special_tokens=[unk_token, '\\n']\n",
    "    )\n",
    "    tokenizer.save(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1b5e6-b22c-4ca6-9d54-e7d999422fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the vocabulary size in the configuration\n",
    "CONFIG['vocab_size'] = tokenizer.get_vocab_size()\n",
    "\n",
    "# Output the total number of characters in our dataset (Vocabulary Size)\n",
    "print(f'Total number of tokens our tokenizer supports: {CONFIG[\"vocab_size\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b64fb-4417-4969-a0a5-0105854a9080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode('Hello world, how are you doing.\\nWhat are you doing?\\nWho are you with?')\n",
    "print(tokenizer.decode(encoded.ids, skip_special_tokens=False))\n",
    "print(encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff6ec5-042f-40e1-92f6-828cfaea33db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# internally, the tokenizer applies the following steps when mapping text to indices\n",
    "print(tokenizer.normalizer)\n",
    "print(tokenizer.pre_tokenizer)\n",
    "print(tokenizer.model)\n",
    "print(tokenizer.post_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca44be7-cb52-4c05-becc-18fe118374e6",
   "metadata": {},
   "source": [
    "# Create pytorch dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea2ca9-ce7b-42dc-8077-2476a323ed32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataset for each split\n",
    "dtype = torch.int16\n",
    "train_dataset = dataset.TextDataset(train_split, tokenizer, CONFIG['context_window'], device, dtype)\n",
    "val_dataset = dataset.TextDataset(val_split, tokenizer, CONFIG['context_window'], device, dtype)\n",
    "test_dataset = dataset.TextDataset(test_split, tokenizer, CONFIG['context_window'], device, dtype)\n",
    "\n",
    "# create a dataloader for each split\n",
    "bs = CONFIG['batch_size']\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "print(f\"Steps train: {len(train_dataloader)}, val: {len(val_dataloader)}, test: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205446e-fc34-43aa-9b4d-19f05ee1a6a9",
   "metadata": {},
   "source": [
    "# Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bdbe11-927c-41b7-a921-02efbccc0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Llama model\n",
    "llama = model.Llama(CONFIG)\n",
    "llama = llama.to(device)\n",
    "model.print_model_parameters(llama)\n",
    "\n",
    "# create the corresponding optimizer\n",
    "optimizer = torch.optim.Adam(llama.parameters(), lr=1e-3)\n",
    "\n",
    "# create a step learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbaba7a-b49e-444a-b5d9-2135afb42c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_losses = training.train(llama, optimizer, train_dataloader, val_dataloader, CONFIG['epochs'], lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2927908-faa5-4a64-8a4b-ce4fbbe64f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "df_losses[['train', 'val']].plot(ax=ax1)\n",
    "df_losses[['lr']].plot(ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d28ddf-9e41-4f3d-a70d-4e3856513419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# check loss on test split\n",
    "training.evaluate_loss(llama, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c861d3-d62e-44ff-b2be-fd9a661680fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate text using the trained LLM (llama) with a maximum of 500 tokens\n",
    "generated_text = llama.generate(device, tokenizer, {'skip_special_tokens': False}, max_new_tokens=500)\n",
    "# since the tokenizer always adds a whitespace before the start of a word, we want to remove the whitespace before the first word of a sentence\n",
    "print(generated_text.replace('\\n ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef857b-3831-4938-ae53-5f8d99d8e649",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7dad92-24f7-4cd7-89d1-a495108da563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the entire model\n",
    "torch.save(llama, os.path.join(experiment_dir, 'llama.pth'))\n",
    "\n",
    "# save only the model parameters\n",
    "# torch.save(llama.state_dict(), os.path.join(experiment_dir, 'llama_model_parameters.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b364360-c4f6-4d22-897b-79dc862bc00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check loaded model\n",
    "llama_loaded = torch.load(os.path.join(experiment_dir, 'llama.pth'))\n",
    "\n",
    "print(llama_loaded.generate(device, tokenizer, {'skip_special_tokens': False}, max_new_tokens=500).replace('\\n ', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c1b95-0498-4cb2-93a9-dfbddb68bbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44776c9-52fc-40b6-b6dd-7a0fd59e72b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
